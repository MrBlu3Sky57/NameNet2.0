{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "4fd97d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from net import Tensor, Network, Embedding, Recurrent, Vanilla, MLP, char_tokenize, stoi, itos, sgd, parse_txt\n",
    "from net.util import SEED\n",
    "\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "995e5a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BLOCK_SIZE = 3\n",
    "FEATURES = 32\n",
    "VOCAB_SIZE = 27\n",
    "HIDDEN_SIZE = 64\n",
    "size = (HIDDEN_SIZE, 128, VOCAB_SIZE)\n",
    "LR = 0.1\n",
    "STEPS = 10000\n",
    "BATCH_SIZE = 128\n",
    "TEMPERATURE = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "1af60d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name data\n",
    "names = parse_txt(\"../data/names.txt\")\n",
    "xs, ys, vocab = char_tokenize(names, BLOCK_SIZE)\n",
    "str_to_int = stoi(vocab)\n",
    "int_to_str = itos(vocab)\n",
    "\n",
    "\n",
    "# Splits\n",
    "b1 = math.floor(len(xs) * 0.8)\n",
    "b2 = math.floor(len(xs) * 0.9)\n",
    "x_train = Tensor(xs[:b1])\n",
    "y_train = Tensor(ys[:b1], grad_required=False)\n",
    "x_test = Tensor(xs[b1:b2], grad_required=False)\n",
    "y_test = Tensor(ys[b1:b2], grad_required=False)\n",
    "x_dev = Tensor(xs[b2:])\n",
    "y_dev = Tensor(ys[b2:], grad_required=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "c8960ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = Embedding(FEATURES, VOCAB_SIZE)\n",
    "cell = Vanilla(HIDDEN_SIZE, FEATURES)\n",
    "rnn = Recurrent(cell)\n",
    "mlp = MLP(size, 'tanh')\n",
    "model = Network([emb, rnn, mlp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "e0840ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.415642953473757 on step: 1\n",
      "Loss: 2.3356541100472836 on step: 101\n",
      "Loss: 2.1422598807628024 on step: 201\n",
      "Loss: 2.0225547344157726 on step: 301\n",
      "Loss: 2.455628622132769 on step: 401\n",
      "Loss: 2.126248081684291 on step: 501\n",
      "Loss: 2.381793575977924 on step: 601\n",
      "Loss: 2.130827288401496 on step: 701\n",
      "Loss: 2.2229562423225335 on step: 801\n",
      "Loss: 2.0321112520848614 on step: 901\n",
      "Loss: 2.0335286462055246 on step: 1001\n",
      "Loss: 2.2134474695272277 on step: 1101\n",
      "Loss: 2.188841525177631 on step: 1201\n",
      "Loss: 2.2080059349760983 on step: 1301\n",
      "Loss: 2.382658257780363 on step: 1401\n",
      "Loss: 2.2479546940149895 on step: 1501\n",
      "Loss: 2.3816655689237747 on step: 1601\n",
      "Loss: 2.187193901803687 on step: 1701\n",
      "Loss: 2.145356558399481 on step: 1801\n",
      "Loss: 2.4684136907357375 on step: 1901\n",
      "Loss: 2.3798719573402813 on step: 2001\n",
      "Loss: 2.2560180469249276 on step: 2101\n",
      "Loss: 2.2947701608076803 on step: 2201\n",
      "Loss: 2.0876094801206593 on step: 2301\n",
      "Loss: 2.357226536608525 on step: 2401\n",
      "Loss: 2.2585861371652416 on step: 2501\n",
      "Loss: 2.1124853941751494 on step: 2601\n",
      "Loss: 2.3483105583045485 on step: 2701\n",
      "Loss: 2.122654797404587 on step: 2801\n",
      "Loss: 2.30930215595987 on step: 2901\n",
      "Loss: 2.3800346980729796 on step: 3001\n",
      "Loss: 2.258269199230687 on step: 3101\n",
      "Loss: 2.316170102427051 on step: 3201\n",
      "Loss: 2.2112587333167144 on step: 3301\n",
      "Loss: 2.356372506375791 on step: 3401\n",
      "Loss: 2.302430437694812 on step: 3501\n",
      "Loss: 2.1542591499118453 on step: 3601\n",
      "Loss: 2.2860599935916937 on step: 3701\n",
      "Loss: 2.2866903974809687 on step: 3801\n",
      "Loss: 2.088717212977803 on step: 3901\n",
      "Loss: 2.3147128797574648 on step: 4001\n",
      "Loss: 2.2923606050413703 on step: 4101\n",
      "Loss: 2.2641953949226137 on step: 4201\n",
      "Loss: 2.171679620333064 on step: 4301\n",
      "Loss: 2.251917082685264 on step: 4401\n",
      "Loss: 2.057807439913543 on step: 4501\n",
      "Loss: 2.0821072993852665 on step: 4601\n",
      "Loss: 2.334198449491092 on step: 4701\n",
      "Loss: 2.339967247005512 on step: 4801\n",
      "Loss: 2.066007410526172 on step: 4901\n",
      "Loss: 2.1956211482424504 on step: 5001\n",
      "Loss: 2.417533964485102 on step: 5101\n",
      "Loss: 2.3047617552469664 on step: 5201\n",
      "Loss: 2.2505999272517903 on step: 5301\n",
      "Loss: 2.2826005128374467 on step: 5401\n",
      "Loss: 2.2247811795611403 on step: 5501\n",
      "Loss: 2.534454765404517 on step: 5601\n",
      "Loss: 2.375288591031567 on step: 5701\n",
      "Loss: 2.2011627064391077 on step: 5801\n",
      "Loss: 2.31692219192215 on step: 5901\n",
      "Loss: 2.3648772088715266 on step: 6001\n",
      "Loss: 2.38146379412605 on step: 6101\n",
      "Loss: 2.4405565197576355 on step: 6201\n",
      "Loss: 2.2994046007715934 on step: 6301\n",
      "Loss: 2.2805457005341014 on step: 6401\n",
      "Loss: 2.1564853504617982 on step: 6501\n",
      "Loss: 2.2493907943939844 on step: 6601\n",
      "Loss: 2.239476265636085 on step: 6701\n",
      "Loss: 2.244852892673787 on step: 6801\n",
      "Loss: 2.1822155719665073 on step: 6901\n",
      "Loss: 2.425145239194094 on step: 7001\n",
      "Loss: 2.2667381811475393 on step: 7101\n",
      "Loss: 2.139423247296149 on step: 7201\n",
      "Loss: 2.1522906941681885 on step: 7301\n",
      "Loss: 2.2072727591795043 on step: 7401\n",
      "Loss: 1.9819975324135135 on step: 7501\n",
      "Loss: 2.138718742813564 on step: 7601\n",
      "Loss: 2.1803011274360506 on step: 7701\n",
      "Loss: 2.1613750849684266 on step: 7801\n",
      "Loss: 2.4074997559587668 on step: 7901\n",
      "Loss: 2.256357911992925 on step: 8001\n",
      "Loss: 2.312626095838271 on step: 8101\n",
      "Loss: 2.129666375387977 on step: 8201\n",
      "Loss: 2.355401651780803 on step: 8301\n",
      "Loss: 2.0427160394580213 on step: 8401\n",
      "Loss: 2.359190458116231 on step: 8501\n",
      "Loss: 2.334046570388135 on step: 8601\n",
      "Loss: 2.254160544941757 on step: 8701\n",
      "Loss: 2.312203450159819 on step: 8801\n",
      "Loss: 2.165542588122029 on step: 8901\n",
      "Loss: 2.1905388120402796 on step: 9001\n",
      "Loss: 2.0846156166779117 on step: 9101\n",
      "Loss: 2.158715984537497 on step: 9201\n",
      "Loss: 2.2891631282055713 on step: 9301\n",
      "Loss: 1.9836765654358557 on step: 9401\n",
      "Loss: 2.1416219340010754 on step: 9501\n",
      "Loss: 2.1166673139601784 on step: 9601\n",
      "Loss: 2.224388579760361 on step: 9701\n",
      "Loss: 2.2540113772159502 on step: 9801\n",
      "Loss: 2.2259505215359905 on step: 9901\n"
     ]
    }
   ],
   "source": [
    "model.training = True\n",
    "sgd(model, x_train, y_train, LR * 0.01, BATCH_SIZE, STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "a7a03beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3320191794219136"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check entropy\n",
    "model.training = False\n",
    "model.forward(x_test)\n",
    "model.loss(y_test).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "b2225554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amarie\n",
      "Marah\n",
      "Jaya\n",
      "Anna\n",
      "Jaylee\n",
      "Jamillia\n",
      "Jani\n",
      "Kayla\n",
      "Kalena\n",
      "Kallie\n",
      "Merya\n",
      "Ala\n",
      "Kendelina\n",
      "Ari\n",
      "Jennah\n",
      "Kenso\n",
      "Amora\n",
      "Amille\n",
      "Aley\n",
      "Ariah\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Similarity score to avoid dups\n",
    "def is_similar(a, b, threshold=0.8):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() > threshold\n",
    "\n",
    "\n",
    "j = 0\n",
    "prev = set()\n",
    "while j < 20:\n",
    "    input = [str_to_int['.']] * BLOCK_SIZE\n",
    "    out = \"\"\n",
    "    while True:\n",
    "        x = Tensor(np.array(input), grad_required=False)\n",
    "        model.forward(x)\n",
    "        logits = model.blocks[-1].out.value.flatten()\n",
    "\n",
    "        # Temperature and Top-k sampling\n",
    "        logits = logits / 0.8\n",
    "        logits -= np.max(logits)  # stabilize\n",
    "        exp_logits = np.exp(logits)\n",
    "        probs = exp_logits / exp_logits.sum()\n",
    "\n",
    "        # Top-k\n",
    "        top_k = 4\n",
    "        top_indices = np.argsort(probs)[-top_k:]\n",
    "        top_probs = probs[top_indices]\n",
    "        top_probs /= top_probs.sum()\n",
    "\n",
    "        i = np.random.choice(top_indices, p=top_probs)\n",
    "        ch = int_to_str[i]\n",
    "\n",
    "        if ch == '.':\n",
    "            break\n",
    "\n",
    "        out += ch\n",
    "        input = input[1:] + [i]\n",
    "\n",
    "    if not any(is_similar(out, word) for word in prev):\n",
    "        print(out.capitalize())\n",
    "        j += 1\n",
    "        prev.add(out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
